"""DSPy signatures and modules used by the evaluator."""

import dspy


class EvalSignature(dspy.Signature):
    """A refined signature that takes a complete user request and uses a separate outcome for validation."""

    request = dspy.InputField(
        desc="The user's full request, including all instructions, context, and relevant code snippets."
    )
    guidelines = dspy.InputField(
        desc="Optional long-form documentation or rules to follow as background context."
    )
    outcome = dspy.InputField(
        desc="The expected result or success criteria. This is used for VALIDATING the answer, not generating it."
    )

    answer = dspy.OutputField(
        desc="The final, complete answer that directly fulfills the request."
    )

class QualityGrader(dspy.Signature):
    """
    Grades a generated answer against a reference answer, based on a key principle and the original task.
    """
    key_principle = dspy.InputField(desc="The key principle or rule the test case is designed to validate.")
    task_requirements = dspy.InputField(desc="The original user request and code to be analyzed.")
    reference_answer = dspy.InputField(desc="The ground truth or ideal answer.")
    generated_answer = dspy.InputField(desc="The answer generated by the model being evaluated.")
    
    score = dspy.OutputField(desc="A single float score between 0.0 and 1.0.")
    reasoning = dspy.OutputField(desc="A brief explanation for the grade, focusing on how well the key principle was applied.")

class CodeComparisonJudge(dspy.Signature):
    """
    Compares generated code against reference code based on task requirements and a specific outcome.
    Outputs a direct score and reasoning.
    """
    
    outcome = dspy.InputField(desc="The key principle or pattern the test case is designed to validate.")
    task_requirements = dspy.InputField(desc="The original requirements for the code generation task.")
    reference_code = dspy.InputField(desc="The ground truth or ideal code implementation from the test file.")
    generated_code = dspy.InputField(desc="The code generated by the model being evaluated.")
    
    score = dspy.OutputField(desc="A single float score between 0.0 and 1.0 evaluating the generated code's adherence to the outcome.")
    reasoning = dspy.OutputField(desc="A brief explanation for the given score, focusing on how well the outcome was achieved.")

class EvaluationModule(dspy.Module):
    """A generic evaluation module that can use any provided signature."""

    def __init__(self, signature_class: dspy.Signature):
        super().__init__()
        self.predictor = dspy.Predict(signature_class)

    def forward(self, test_case_id: str = None, **kwargs):
        """Forwards the request to the configured model's execution method."""
        return dspy.settings.lm.execute_prediction(self, test_case_id=test_case_id, **kwargs)
