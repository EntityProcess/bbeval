"""DSPy signatures and modules used by the evaluator."""

import dspy


class EvalSignature(dspy.Signature):
    """A refined signature that takes a complete user request and uses a separate outcome for validation."""

    request = dspy.InputField(
        desc="The user's full request, including all instructions, context, and relevant code snippets."
    )
    guidelines = dspy.InputField(
        desc="Optional long-form documentation or rules to follow as background context."
    )
    outcome = dspy.InputField(
        desc="The expected result or success criteria. This is used for VALIDATING the answer, not generating it."
    )

    answer = dspy.OutputField(
        desc="The final, complete answer that directly fulfills the request."
    )

class QualityGrader(dspy.Signature):
    """
    Grades a generated answer against a reference answer, based on a key principle and the original task.
    """
    expected_outcome = dspy.InputField(desc="The key principle or rule the test case is designed to validate.")
    request = dspy.InputField(desc="The original user request and code to be analyzed.")
    reference_answer = dspy.InputField(desc="The ground truth or ideal answer.")
    generated_answer = dspy.InputField(desc="The answer generated by the model being evaluated.")
    
    score = dspy.OutputField(desc="A single float score between 0.0 and 1.0.")
    reasoning = dspy.OutputField(desc="A brief explanation for the grade, focusing on how well the key principle was applied.")

class EvaluationModule(dspy.Module):
    """A generic evaluation module that can use any provided signature."""

    def __init__(self, signature_class: dspy.Signature):
        super().__init__()
        self.predictor = dspy.Predict(signature_class)

    def forward(self, test_case_id: str = None, **kwargs):
        """Forwards the request to the configured model's execution method."""
        return dspy.settings.lm.execute_prediction(self, test_case_id=test_case_id, **kwargs)
